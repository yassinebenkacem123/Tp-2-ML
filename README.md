# ğŸ§  TP2 â€“ Apprentissage Profond avec TensorFlow / Keras
**Module : Technologies de lâ€™Intelligence Artificielle**  
**Encadrant : Pr. Youness IDRISSI KHAMLICHI**  
**FiliÃ¨re : IngÃ©nierie Logicielle et Intelligence Artificielle â€“ ILIA2**  
**Ã‰cole : ENSA FÃ¨s**

---

## ğŸ¯ Objectif du TP

Ce TP a pour but dâ€™introduire les concepts fondamentaux de lâ€™**apprentissage profond (Deep Learning)** Ã  travers la mise en Å“uvre de rÃ©seaux de neurones artificiels avec **TensorFlow** et **Keras**.  
Les Ã©tudiants apprendront Ã  construire, entraÃ®ner et Ã©valuer des modÃ¨les capables de reconnaÃ®tre des images issues des jeux de donnÃ©es **MNIST** et **Fashion-MNIST**.

---

## ğŸ“˜ Contenu du TP

### ğŸ”¹ Exercice 1 â€“ Reconnaissance de chiffres manuscrits (MNIST)

**But :** Construire un rÃ©seau de neurones entiÃ¨rement connectÃ© (Dense Neural Network) pour reconnaÃ®tre les chiffres manuscrits de 0 Ã  9.

**Dataset :** [MNIST â€“ Kaggle](https://www.kaggle.com/datasets/hojjatk/mnist-dataset)  
**Taille :** 70 000 images (28Ã—28 pixels, niveaux de gris)

#### Ã‰tapes :
1. Charger et normaliser les donnÃ©es  
2. Visualiser quelques exemples dâ€™images  
3. CrÃ©er un modÃ¨le `Sequential` avec les couches :
   - `Flatten()` pour transformer les images 2D en vecteurs 1D  
   - `Dense(128, activation='relu')`  
   - `Dense(10, activation='softmax')`
4. Compiler le modÃ¨le avec :
   - Optimiseur : `adam`  
   - Fonction de perte : `sparse_categorical_crossentropy`  
   - MÃ©trique : `accuracy`
5. EntraÃ®ner le modÃ¨le sur 5 Ã©poques  
6. Ã‰valuer les performances sur le jeu de test  
7. Visualiser les courbes dâ€™apprentissage (accuracy et loss)  
8. Afficher quelques prÃ©dictions sur des images tests  

---

### ğŸ”¹ Exercice 2 â€“ Classification dâ€™images : Fashion-MNIST

**But :** DÃ©couvrir le **Deep Learning** Ã  travers la classification dâ€™images de vÃªtements (T-shirt, pantalon, robe, etc.).

**Dataset :** [Fashion-MNIST â€“ Kaggle](https://www.kaggle.com/datasets/zalando-research/fashionmnist)  
**Taille :** 70 000 images (60 000 train, 10 000 test)

#### Ã‰tapes principales :
1. **Exploration** du dataset et visualisation dâ€™exemples  
2. **PrÃ©traitement :**  
   - Normalisation des pixels entre 0 et 1  
   - Encodage des labels (one-hot encoding)
3. **ModÃ¨le Dense (ANN)** :  
   - EntraÃ®nement et Ã©valuation du modÃ¨le
4. **Comparaison des fonctions dâ€™activation** : ReLU, Sigmoid, Tanh, LeakyReLU  
5. **CrÃ©ation dâ€™un modÃ¨le CNN (Convolutional Neural Network)** pour amÃ©liorer la prÃ©cision  
6. **Visualisation des performances** et analyse des erreurs  
7. **Data Augmentation** pour enrichir les donnÃ©es dâ€™entraÃ®nement

---

## ğŸ§ª Concepts clÃ©s abordÃ©s

| Concept | Description |
|----------|--------------|
| **Neurone** | UnitÃ© de base dâ€™un rÃ©seau, effectuant un calcul pondÃ©rÃ© puis une activation. |
| **Couche (Layer)** | Regroupe plusieurs neurones traitant une reprÃ©sentation des donnÃ©es. |
| **Fonction dâ€™activation** | Introduit la non-linÃ©aritÃ© (ReLU, Sigmoid, Softmax). |
| **Fonction de perte** | Mesure la diffÃ©rence entre la prÃ©diction et la valeur rÃ©elle. |
| **Optimiseur** | MÃ©thode dâ€™ajustement des poids (Adam, SGD, RMSprop). |
| **Ã‰poque (Epoch)** | Une itÃ©ration complÃ¨te sur toutes les donnÃ©es dâ€™entraÃ®nement. |
| **Batch** | Sous-ensemble des donnÃ©es utilisÃ© Ã  chaque itÃ©ration. |
| **RÃ©tropropagation** | Ajustement des poids selon lâ€™erreur obtenue. |

